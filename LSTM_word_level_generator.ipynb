{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Activation,  Dropout, Dense, Embedding, Bidirectional\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import keras.utils as ku \n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pickle\n",
    "\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/glep93/X-mas_song_generator/main/xmas-songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x):\n",
    "    x = x.lower()\n",
    "    x = re.sub('\\[(.*?)\\]', '', x)\n",
    "    x = re.sub('[\\n]{2,}', '\\n',x)\n",
    "    x = re.sub(\"[^a-z\\'\\s\\n]\",'',x)\n",
    "    x = re.sub('[\\n]', ' *newline* ',x)\n",
    "    x = re.sub('^\\s','',x)\n",
    "    x = re.sub('[\\s]{2,}', ' ',x)\n",
    "    out =np.array( x.split(' ') )\n",
    "    out = out[out!= '']\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics =df['lyrics'].dropna()[ df.Artist=='Michael Bubl√©'][[13,144 ,170 ,289 ,387 ,592]]\n",
    "lyrics = lyrics.apply(preprocessing )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387\n"
     ]
    }
   ],
   "source": [
    "aux = []\n",
    "for i in lyrics:\n",
    "    aux  = np.append(aux, i)\n",
    "\n",
    "vocab = set( list(aux) )\n",
    "\n",
    "index_to_word = { i+1: word for i, word in enumerate(vocab) }\n",
    "word_to_index = {  word: i+1 for i, word in enumerate(vocab) }\n",
    "index_to_word[0] = 'missing_word'\n",
    "word_to_index['missing_word'] = 0\n",
    "\n",
    "total_words = len( vocab)+1\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( index_to_word, open( \"index_to_word.p\", \"wb\" ) )\n",
    "pickle.dump( word_to_index, open( \"word_to_index.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for song in lyrics:\n",
    "    N = len(song)\n",
    "    for i in range( N -n- 1):\n",
    "        for j in range(1,n):\n",
    "            X.append(  [word_to_index[word]  for word in song[i:i+j]] )\n",
    "            y.append( word_to_index[song[i+j]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133429, 30)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad = pad_sequences(X, maxlen = n, padding='pre')\n",
    "X_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample_training = 100000\n",
    "tot=len(X_pad)\n",
    "index_training=np.random.randint(0,tot,num_sample_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_pad[index_training]\n",
    "y_train = y[index_training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 50)            69350     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 30, 256)           183296    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 693)               89397     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1387)              962578    \n",
      "=================================================================\n",
      "Total params: 1,501,741\n",
      "Trainable params: 1,501,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add Input Embedding Layer\n",
    "model.add(Embedding(total_words, 50, input_length=30))\n",
    "    \n",
    " # Add Hidden Layer 1 - LSTM Layer\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "#model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(total_words/2, activation='relu')) \n",
    "\n",
    "# Add Output Layer\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ku.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 2.1662\n",
      "Epoch 00001: loss improved from inf to 2.16622, saving model to weights-improvement-01-2.1662.hdf5\n",
      "261/261 [==============================] - 124s 477ms/step - loss: 2.1662\n",
      "Epoch 2/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 1.3819\n",
      "Epoch 00002: loss improved from 2.16622 to 1.38188, saving model to weights-improvement-02-1.3819.hdf5\n",
      "261/261 [==============================] - 151s 578ms/step - loss: 1.3819\n",
      "Epoch 3/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 0.9261\n",
      "Epoch 00003: loss improved from 1.38188 to 0.92605, saving model to weights-improvement-03-0.9261.hdf5\n",
      "261/261 [==============================] - 140s 535ms/step - loss: 0.9261\n",
      "Epoch 4/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 0.5943\n",
      "Epoch 00004: loss improved from 0.92605 to 0.59434, saving model to weights-improvement-04-0.5943.hdf5\n",
      "261/261 [==============================] - 148s 568ms/step - loss: 0.5943\n",
      "Epoch 5/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 0.4587\n",
      "Epoch 00005: loss improved from 0.59434 to 0.45873, saving model to weights-improvement-05-0.4587.hdf5\n",
      "261/261 [==============================] - 152s 582ms/step - loss: 0.4587\n",
      "Epoch 6/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 0.3704\n",
      "Epoch 00006: loss improved from 0.45873 to 0.37043, saving model to weights-improvement-06-0.3704.hdf5\n",
      "261/261 [==============================] - 145s 555ms/step - loss: 0.3704\n",
      "Epoch 7/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 0.3249\n",
      "Epoch 00007: loss improved from 0.37043 to 0.32489, saving model to weights-improvement-07-0.3249.hdf5\n",
      "261/261 [==============================] - 185s 708ms/step - loss: 0.3249\n",
      "Epoch 8/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 0.2948\n",
      "Epoch 00008: loss improved from 0.32489 to 0.29476, saving model to weights-improvement-08-0.2948.hdf5\n",
      "261/261 [==============================] - 198s 760ms/step - loss: 0.2948\n",
      "Epoch 9/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 0.6673\n",
      "Epoch 00009: loss did not improve from 0.29476\n",
      "261/261 [==============================] - 131s 502ms/step - loss: 0.6673\n",
      "Epoch 10/10\n",
      "261/261 [==============================] - ETA: 0s - loss: 0.2614\n",
      "Epoch 00010: loss improved from 0.29476 to 0.26142, saving model to weights-improvement-10-0.2614.hdf5\n",
      "261/261 [==============================] - 116s 443ms/step - loss: 0.2614\n"
     ]
    }
   ],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "history = model.fit(X_pad, label, epochs=10, batch_size=512, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luca/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/luca/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: mAIchael_buble/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('mAIchael_buble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model_load = keras.models.load_model('mAIchael_buble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lyrics( input, n_words):\n",
    "    input_p = preprocessing(input)\n",
    "    for i in range(n_words):\n",
    "        input_pad = pad_sequences( [[word_to_index[word] if word in word_to_index.keys() else 0  for word in input_p ]] , maxlen = n, padding='pre')\n",
    "        predict = model_load.predict_classes(input_pad, verbose=0)[0]\n",
    "        if index_to_word[predict] == '*newline*':\n",
    "            input = input + '\\n'\n",
    "        else:\n",
    "            input = input +' '+ index_to_word[predict] \n",
    "        input_p = np.append(input_p, index_to_word[predict])\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-57-3b8d75006a8c>:5: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "all i want for christmas\n",
      " is you\n",
      " i won't ask for much this christmas\n",
      " i won't even wish for snow\n",
      " no i'm just gonna keep on waiting\n",
      " underneath the mistletoe\n",
      " there's no sense in hanging stockings\n",
      " there upon the fireplace\n",
      " 'cause santa he won't make me happy\n",
      " with a toy on christmas day\n",
      " i just want you here tonight\n",
      " holding on to me so tight\n",
      " girl what can i do\n",
      " you know that all i want for christmas\n",
      " is you\n",
      " and all the lights are shining\n",
      " so brightly everywhere\n",
      " and the sound of children's\n",
      " laughter fills the air\n",
      " and everyone is singing\n",
      " i can hear those sleigh bells ringing\n",
      " santa won't you bring me the one i really love\n",
      " won't you please bring my baby to me\n",
      " i don't want a lot for christmas\n",
      " this is all i'm asking for\n",
      " no i just wanna see my baby\n",
      " standing right outside my door\n",
      " oh i just want you for my own\n",
      " more that you could ever know\n",
      " make my wish come true\n",
      " you know that all\n"
     ]
    }
   ],
   "source": [
    "input  = '''all i want for christmas'''\n",
    "\n",
    "print(make_lyrics( input, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
